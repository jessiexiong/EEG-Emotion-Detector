{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514a949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39bddac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-632.55413</td>\n",
       "      <td>-464.77713</td>\n",
       "      <td>-319.84153</td>\n",
       "      <td>-313.30638</td>\n",
       "      <td>36.27217</td>\n",
       "      <td>-13.55798</td>\n",
       "      <td>0.63779</td>\n",
       "      <td>0.84635</td>\n",
       "      <td>-448.93280</td>\n",
       "      <td>-438.11205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02715</td>\n",
       "      <td>0.02815</td>\n",
       "      <td>0.00581</td>\n",
       "      <td>0.00310</td>\n",
       "      <td>0.00226</td>\n",
       "      <td>0.00387</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.00649</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.36622</td>\n",
       "      <td>9.02563</td>\n",
       "      <td>9.31294</td>\n",
       "      <td>9.59220</td>\n",
       "      <td>-3.66945</td>\n",
       "      <td>-6.77996</td>\n",
       "      <td>-1.01670</td>\n",
       "      <td>-3.00561</td>\n",
       "      <td>4.23396</td>\n",
       "      <td>8.16198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02265</td>\n",
       "      <td>0.03048</td>\n",
       "      <td>0.00776</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00455</td>\n",
       "      <td>0.00222</td>\n",
       "      <td>0.00158</td>\n",
       "      <td>0.00393</td>\n",
       "      <td>0.00287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.84970</td>\n",
       "      <td>-5.97114</td>\n",
       "      <td>2.00823</td>\n",
       "      <td>4.37346</td>\n",
       "      <td>-16.88866</td>\n",
       "      <td>15.18927</td>\n",
       "      <td>14.09191</td>\n",
       "      <td>-0.56374</td>\n",
       "      <td>34.90172</td>\n",
       "      <td>-13.68915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.00547</td>\n",
       "      <td>0.00311</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>0.00325</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-395.78789</td>\n",
       "      <td>-282.40165</td>\n",
       "      <td>-178.44059</td>\n",
       "      <td>-174.95711</td>\n",
       "      <td>-243.71179</td>\n",
       "      <td>-321.40315</td>\n",
       "      <td>-159.32955</td>\n",
       "      <td>-157.94618</td>\n",
       "      <td>-298.46577</td>\n",
       "      <td>-146.89583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02920</td>\n",
       "      <td>0.03735</td>\n",
       "      <td>0.01385</td>\n",
       "      <td>0.00849</td>\n",
       "      <td>0.00618</td>\n",
       "      <td>0.00388</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00273</td>\n",
       "      <td>0.00216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-469.88683</td>\n",
       "      <td>-614.04482</td>\n",
       "      <td>-333.31084</td>\n",
       "      <td>-328.16136</td>\n",
       "      <td>0.13096</td>\n",
       "      <td>5.13976</td>\n",
       "      <td>0.13841</td>\n",
       "      <td>1.52704</td>\n",
       "      <td>-470.51339</td>\n",
       "      <td>-622.11413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.08236</td>\n",
       "      <td>0.02827</td>\n",
       "      <td>0.01588</td>\n",
       "      <td>0.01094</td>\n",
       "      <td>0.00847</td>\n",
       "      <td>0.00354</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>0.00665</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>58.77315</td>\n",
       "      <td>10.79600</td>\n",
       "      <td>12.32300</td>\n",
       "      <td>-4.79704</td>\n",
       "      <td>-25.10559</td>\n",
       "      <td>-17.30562</td>\n",
       "      <td>-7.61847</td>\n",
       "      <td>-5.72821</td>\n",
       "      <td>85.90159</td>\n",
       "      <td>33.92513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03610</td>\n",
       "      <td>0.05159</td>\n",
       "      <td>0.02053</td>\n",
       "      <td>0.00995</td>\n",
       "      <td>0.00545</td>\n",
       "      <td>0.00529</td>\n",
       "      <td>0.00670</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>0.00153</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>41.90976</td>\n",
       "      <td>-96.90947</td>\n",
       "      <td>-6.60463</td>\n",
       "      <td>-10.83695</td>\n",
       "      <td>-19.99803</td>\n",
       "      <td>-17.71022</td>\n",
       "      <td>-11.18951</td>\n",
       "      <td>-10.05006</td>\n",
       "      <td>76.43298</td>\n",
       "      <td>-71.50214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13063</td>\n",
       "      <td>0.18636</td>\n",
       "      <td>0.05654</td>\n",
       "      <td>0.03055</td>\n",
       "      <td>0.02590</td>\n",
       "      <td>0.02013</td>\n",
       "      <td>0.01802</td>\n",
       "      <td>0.01398</td>\n",
       "      <td>0.01111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>-396.44124</td>\n",
       "      <td>-303.40478</td>\n",
       "      <td>-182.05478</td>\n",
       "      <td>-178.09819</td>\n",
       "      <td>2.14805</td>\n",
       "      <td>2.46233</td>\n",
       "      <td>1.82104</td>\n",
       "      <td>1.04799</td>\n",
       "      <td>-399.09244</td>\n",
       "      <td>-304.82338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02224</td>\n",
       "      <td>0.13425</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00497</td>\n",
       "      <td>0.00848</td>\n",
       "      <td>0.00599</td>\n",
       "      <td>0.00336</td>\n",
       "      <td>0.00532</td>\n",
       "      <td>0.00882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>-459.54690</td>\n",
       "      <td>-402.50104</td>\n",
       "      <td>-237.07238</td>\n",
       "      <td>-232.48756</td>\n",
       "      <td>-33.12614</td>\n",
       "      <td>-10.18965</td>\n",
       "      <td>-13.69952</td>\n",
       "      <td>-12.15816</td>\n",
       "      <td>-429.96694</td>\n",
       "      <td>-393.01263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00584</td>\n",
       "      <td>0.05130</td>\n",
       "      <td>0.00817</td>\n",
       "      <td>0.00280</td>\n",
       "      <td>0.00404</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>0.00630</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>-494.29815</td>\n",
       "      <td>-477.20640</td>\n",
       "      <td>-400.10940</td>\n",
       "      <td>-395.54488</td>\n",
       "      <td>-3.24169</td>\n",
       "      <td>1.33530</td>\n",
       "      <td>1.50892</td>\n",
       "      <td>0.51470</td>\n",
       "      <td>-495.03600</td>\n",
       "      <td>-481.72349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02210</td>\n",
       "      <td>0.10715</td>\n",
       "      <td>0.00545</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.00816</td>\n",
       "      <td>0.00428</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1176 rows × 989 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "0      -632.55413   -464.77713   -319.84153   -313.30638            36.27217   \n",
       "1        14.36622      9.02563      9.31294      9.59220            -3.66945   \n",
       "2        23.84970     -5.97114      2.00823      4.37346           -16.88866   \n",
       "3      -395.78789   -282.40165   -178.44059   -174.95711          -243.71179   \n",
       "4      -469.88683   -614.04482   -333.31084   -328.16136             0.13096   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "1171     58.77315     10.79600     12.32300     -4.79704           -25.10559   \n",
       "1172     41.90976    -96.90947     -6.60463    -10.83695           -19.99803   \n",
       "1173   -396.44124   -303.40478   -182.05478   -178.09819             2.14805   \n",
       "1174   -459.54690   -402.50104   -237.07238   -232.48756           -33.12614   \n",
       "1175   -494.29815   -477.20640   -400.10940   -395.54488            -3.24169   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "0              -13.55798             0.63779             0.84635   \n",
       "1               -6.77996            -1.01670            -3.00561   \n",
       "2               15.18927            14.09191            -0.56374   \n",
       "3             -321.40315          -159.32955          -157.94618   \n",
       "4                5.13976             0.13841             1.52704   \n",
       "...                  ...                 ...                 ...   \n",
       "1171           -17.30562            -7.61847            -5.72821   \n",
       "1172           -17.71022           -11.18951           -10.05006   \n",
       "1173             2.46233             1.82104             1.04799   \n",
       "1174           -10.18965           -13.69952           -12.15816   \n",
       "1175             1.33530             1.50892             0.51470   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_669_3  freq_679_3  freq_689_3  \\\n",
       "0         -448.93280      -438.11205  ...     0.02715     0.02815     0.00581   \n",
       "1            4.23396         8.16198  ...     0.02265     0.03048     0.00776   \n",
       "2           34.90172       -13.68915  ...     0.00140     0.00547     0.00311   \n",
       "3         -298.46577      -146.89583  ...     0.02920     0.03735     0.01385   \n",
       "4         -470.51339      -622.11413  ...     0.05366     0.08236     0.02827   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "1171        85.90159        33.92513  ...     0.03610     0.05159     0.02053   \n",
       "1172        76.43298       -71.50214  ...     0.13063     0.18636     0.05654   \n",
       "1173      -399.09244      -304.82338  ...     0.02224     0.13425     0.01508   \n",
       "1174      -429.96694      -393.01263  ...     0.00584     0.05130     0.00817   \n",
       "1175      -495.03600      -481.72349  ...     0.02210     0.10715     0.00545   \n",
       "\n",
       "      freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  freq_750_3  \\\n",
       "0        0.00310     0.00226     0.00387     0.00181     0.00649     0.00151   \n",
       "1        0.00465     0.00455     0.00222     0.00158     0.00393     0.00287   \n",
       "2        0.00141     0.00155     0.00345     0.00325     0.00245     0.00136   \n",
       "3        0.00849     0.00618     0.00388     0.00396     0.00273     0.00216   \n",
       "4        0.01588     0.01094     0.00847     0.00354     0.00439     0.00665   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1171     0.00995     0.00545     0.00529     0.00670     0.00384     0.00153   \n",
       "1172     0.03055     0.02590     0.02013     0.01802     0.01398     0.01111   \n",
       "1173     0.00497     0.00848     0.00599     0.00336     0.00532     0.00882   \n",
       "1174     0.00280     0.00404     0.00449     0.00438     0.00119     0.00630   \n",
       "1175     0.00813     0.00816     0.00428     0.00040     0.00384     0.01966   \n",
       "\n",
       "      Label  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       0.0  \n",
       "4       1.0  \n",
       "...     ...  \n",
       "1171    0.0  \n",
       "1172    0.0  \n",
       "1173    0.0  \n",
       "1174    1.0  \n",
       "1175    0.0  \n",
       "\n",
       "[1176 rows x 989 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/jessiexiong/Desktop/preprocessed/b-test.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ab395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    588\n",
       "0.0    588\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f7b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Relaxed': 0, 'Concentrated': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ffa6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    \n",
    "    y = df['Label'].copy()\n",
    "    X = df.drop('Label', axis=1).copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba9c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248305ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_659_3</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>40.65537</td>\n",
       "      <td>-5.12587</td>\n",
       "      <td>5.40203</td>\n",
       "      <td>-6.70313</td>\n",
       "      <td>2.33137</td>\n",
       "      <td>0.67382</td>\n",
       "      <td>4.12017</td>\n",
       "      <td>-0.17253</td>\n",
       "      <td>45.76809</td>\n",
       "      <td>-1.51215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01653</td>\n",
       "      <td>0.03425</td>\n",
       "      <td>0.04102</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>0.00475</td>\n",
       "      <td>0.00408</td>\n",
       "      <td>0.00764</td>\n",
       "      <td>0.00717</td>\n",
       "      <td>0.00483</td>\n",
       "      <td>0.00812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>-497.11389</td>\n",
       "      <td>-478.40422</td>\n",
       "      <td>-402.11158</td>\n",
       "      <td>-396.27699</td>\n",
       "      <td>-2.51424</td>\n",
       "      <td>-3.66157</td>\n",
       "      <td>-5.49825</td>\n",
       "      <td>-2.05941</td>\n",
       "      <td>-493.48696</td>\n",
       "      <td>-475.04153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02423</td>\n",
       "      <td>0.06364</td>\n",
       "      <td>0.08555</td>\n",
       "      <td>0.03530</td>\n",
       "      <td>0.01526</td>\n",
       "      <td>0.01802</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>0.00777</td>\n",
       "      <td>0.01077</td>\n",
       "      <td>0.00837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>24.49995</td>\n",
       "      <td>20.34522</td>\n",
       "      <td>33.43521</td>\n",
       "      <td>25.78797</td>\n",
       "      <td>-19.71613</td>\n",
       "      <td>-41.09932</td>\n",
       "      <td>26.42427</td>\n",
       "      <td>15.30655</td>\n",
       "      <td>32.11592</td>\n",
       "      <td>45.79592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05626</td>\n",
       "      <td>0.16943</td>\n",
       "      <td>0.21449</td>\n",
       "      <td>0.07266</td>\n",
       "      <td>0.04012</td>\n",
       "      <td>0.02999</td>\n",
       "      <td>0.02184</td>\n",
       "      <td>0.01190</td>\n",
       "      <td>0.01163</td>\n",
       "      <td>0.01882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>55.94245</td>\n",
       "      <td>36.10842</td>\n",
       "      <td>26.65242</td>\n",
       "      <td>24.18881</td>\n",
       "      <td>-8.68223</td>\n",
       "      <td>-6.05912</td>\n",
       "      <td>-3.47516</td>\n",
       "      <td>-2.35183</td>\n",
       "      <td>55.81424</td>\n",
       "      <td>30.77370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12220</td>\n",
       "      <td>0.31399</td>\n",
       "      <td>0.42398</td>\n",
       "      <td>0.11817</td>\n",
       "      <td>0.07980</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.04571</td>\n",
       "      <td>0.02570</td>\n",
       "      <td>0.01967</td>\n",
       "      <td>0.01347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-400.01029</td>\n",
       "      <td>-303.67819</td>\n",
       "      <td>-189.38979</td>\n",
       "      <td>-185.36353</td>\n",
       "      <td>-0.28635</td>\n",
       "      <td>-2.72801</td>\n",
       "      <td>-0.04381</td>\n",
       "      <td>0.25425</td>\n",
       "      <td>-398.31062</td>\n",
       "      <td>-300.78970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02770</td>\n",
       "      <td>0.07760</td>\n",
       "      <td>0.10679</td>\n",
       "      <td>0.03062</td>\n",
       "      <td>0.02163</td>\n",
       "      <td>0.01207</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>0.00793</td>\n",
       "      <td>0.00489</td>\n",
       "      <td>0.00327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-489.19031</td>\n",
       "      <td>-643.79896</td>\n",
       "      <td>-357.32888</td>\n",
       "      <td>-353.55452</td>\n",
       "      <td>-9.81430</td>\n",
       "      <td>-12.74423</td>\n",
       "      <td>-6.77121</td>\n",
       "      <td>-0.98398</td>\n",
       "      <td>-488.92225</td>\n",
       "      <td>-638.58530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01039</td>\n",
       "      <td>0.01795</td>\n",
       "      <td>0.09397</td>\n",
       "      <td>0.01201</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.00513</td>\n",
       "      <td>0.00041</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.00172</td>\n",
       "      <td>0.00152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>-398.84222</td>\n",
       "      <td>-304.85566</td>\n",
       "      <td>-186.18597</td>\n",
       "      <td>-181.93002</td>\n",
       "      <td>2.60495</td>\n",
       "      <td>0.38443</td>\n",
       "      <td>6.37878</td>\n",
       "      <td>6.51269</td>\n",
       "      <td>-399.00091</td>\n",
       "      <td>-305.02401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01282</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.13715</td>\n",
       "      <td>0.01990</td>\n",
       "      <td>0.00868</td>\n",
       "      <td>0.00677</td>\n",
       "      <td>0.00343</td>\n",
       "      <td>0.00257</td>\n",
       "      <td>0.00270</td>\n",
       "      <td>0.00218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>37.20249</td>\n",
       "      <td>-9.63909</td>\n",
       "      <td>9.90801</td>\n",
       "      <td>-5.40994</td>\n",
       "      <td>-24.20661</td>\n",
       "      <td>-18.67018</td>\n",
       "      <td>3.29271</td>\n",
       "      <td>17.27392</td>\n",
       "      <td>44.82457</td>\n",
       "      <td>-1.58925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03097</td>\n",
       "      <td>0.07372</td>\n",
       "      <td>0.10031</td>\n",
       "      <td>0.03552</td>\n",
       "      <td>0.01864</td>\n",
       "      <td>0.01076</td>\n",
       "      <td>0.01074</td>\n",
       "      <td>0.00893</td>\n",
       "      <td>0.01275</td>\n",
       "      <td>0.00762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>52.95540</td>\n",
       "      <td>-99.00649</td>\n",
       "      <td>-0.57475</td>\n",
       "      <td>-5.04624</td>\n",
       "      <td>4.02571</td>\n",
       "      <td>-19.61873</td>\n",
       "      <td>-2.16663</td>\n",
       "      <td>-2.98198</td>\n",
       "      <td>37.93795</td>\n",
       "      <td>-98.26754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04602</td>\n",
       "      <td>0.11926</td>\n",
       "      <td>0.16711</td>\n",
       "      <td>0.04779</td>\n",
       "      <td>0.02801</td>\n",
       "      <td>0.02170</td>\n",
       "      <td>0.01763</td>\n",
       "      <td>0.01307</td>\n",
       "      <td>0.01277</td>\n",
       "      <td>0.01454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>54.47839</td>\n",
       "      <td>13.92173</td>\n",
       "      <td>11.06293</td>\n",
       "      <td>-1.49479</td>\n",
       "      <td>51.23902</td>\n",
       "      <td>-7.97410</td>\n",
       "      <td>5.32469</td>\n",
       "      <td>-1.92768</td>\n",
       "      <td>61.81782</td>\n",
       "      <td>15.54374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00579</td>\n",
       "      <td>0.00494</td>\n",
       "      <td>0.04845</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>0.00201</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>0.00371</td>\n",
       "      <td>0.00179</td>\n",
       "      <td>0.00249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823 rows × 988 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "246      40.65537     -5.12587      5.40203     -6.70313             2.33137   \n",
       "1009   -497.11389   -478.40422   -402.11158   -396.27699            -2.51424   \n",
       "431      24.49995     20.34522     33.43521     25.78797           -19.71613   \n",
       "102      55.94245     36.10842     26.65242     24.18881            -8.68223   \n",
       "308    -400.01029   -303.67819   -189.38979   -185.36353            -0.28635   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "96     -489.19031   -643.79896   -357.32888   -353.55452            -9.81430   \n",
       "1147   -398.84222   -304.85566   -186.18597   -181.93002             2.60495   \n",
       "106      37.20249     -9.63909      9.90801     -5.40994           -24.20661   \n",
       "1041     52.95540    -99.00649     -0.57475     -5.04624             4.02571   \n",
       "1122     54.47839     13.92173     11.06293     -1.49479            51.23902   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "246              0.67382             4.12017            -0.17253   \n",
       "1009            -3.66157            -5.49825            -2.05941   \n",
       "431            -41.09932            26.42427            15.30655   \n",
       "102             -6.05912            -3.47516            -2.35183   \n",
       "308             -2.72801            -0.04381             0.25425   \n",
       "...                  ...                 ...                 ...   \n",
       "96             -12.74423            -6.77121            -0.98398   \n",
       "1147             0.38443             6.37878             6.51269   \n",
       "106            -18.67018             3.29271            17.27392   \n",
       "1041           -19.61873            -2.16663            -2.98198   \n",
       "1122            -7.97410             5.32469            -1.92768   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_659_3  freq_669_3  freq_679_3  \\\n",
       "246         45.76809        -1.51215  ...     0.01653     0.03425     0.04102   \n",
       "1009      -493.48696      -475.04153  ...     0.02423     0.06364     0.08555   \n",
       "431         32.11592        45.79592  ...     0.05626     0.16943     0.21449   \n",
       "102         55.81424        30.77370  ...     0.12220     0.31399     0.42398   \n",
       "308       -398.31062      -300.78970  ...     0.02770     0.07760     0.10679   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "96        -488.92225      -638.58530  ...     0.01039     0.01795     0.09397   \n",
       "1147      -399.00091      -305.02401  ...     0.01282     0.02232     0.13715   \n",
       "106         44.82457        -1.58925  ...     0.03097     0.07372     0.10031   \n",
       "1041        37.93795       -98.26754  ...     0.04602     0.11926     0.16711   \n",
       "1122        61.81782        15.54374  ...     0.00579     0.00494     0.04845   \n",
       "\n",
       "      freq_689_3  freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  \\\n",
       "246      0.00720     0.00475     0.00408     0.00764     0.00717     0.00483   \n",
       "1009     0.03530     0.01526     0.01802     0.00214     0.00777     0.01077   \n",
       "431      0.07266     0.04012     0.02999     0.02184     0.01190     0.01163   \n",
       "102      0.11817     0.07980     0.05999     0.04571     0.02570     0.01967   \n",
       "308      0.03062     0.02163     0.01207     0.00699     0.00793     0.00489   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "96       0.01201     0.00281     0.00513     0.00041     0.00176     0.00172   \n",
       "1147     0.01990     0.00868     0.00677     0.00343     0.00257     0.00270   \n",
       "106      0.03552     0.01864     0.01076     0.01074     0.00893     0.01275   \n",
       "1041     0.04779     0.02801     0.02170     0.01763     0.01307     0.01277   \n",
       "1122     0.00495     0.00201     0.00139     0.00131     0.00371     0.00179   \n",
       "\n",
       "      freq_750_3  \n",
       "246      0.00812  \n",
       "1009     0.00837  \n",
       "431      0.01882  \n",
       "102      0.01347  \n",
       "308      0.00327  \n",
       "...          ...  \n",
       "96       0.00152  \n",
       "1147     0.00218  \n",
       "106      0.00762  \n",
       "1041     0.01454  \n",
       "1122     0.00249  \n",
       "\n",
       "[823 rows x 988 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae2c0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag1_mean_0</th>\n",
       "      <th>lag1_mean_1</th>\n",
       "      <th>lag1_mean_2</th>\n",
       "      <th>lag1_mean_3</th>\n",
       "      <th>lag1_mean_d_h2h1_0</th>\n",
       "      <th>lag1_mean_d_h2h1_1</th>\n",
       "      <th>lag1_mean_d_h2h1_2</th>\n",
       "      <th>lag1_mean_d_h2h1_3</th>\n",
       "      <th>lag1_mean_q1_0</th>\n",
       "      <th>lag1_mean_q1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>freq_659_3</th>\n",
       "      <th>freq_669_3</th>\n",
       "      <th>freq_679_3</th>\n",
       "      <th>freq_689_3</th>\n",
       "      <th>freq_699_3</th>\n",
       "      <th>freq_709_3</th>\n",
       "      <th>freq_720_3</th>\n",
       "      <th>freq_730_3</th>\n",
       "      <th>freq_740_3</th>\n",
       "      <th>freq_750_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>13.39603</td>\n",
       "      <td>5.96764</td>\n",
       "      <td>8.02252</td>\n",
       "      <td>8.37353</td>\n",
       "      <td>4.15383</td>\n",
       "      <td>1.34713</td>\n",
       "      <td>0.11239</td>\n",
       "      <td>1.34190</td>\n",
       "      <td>16.98297</td>\n",
       "      <td>9.55623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02189</td>\n",
       "      <td>0.04148</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.02114</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>0.01481</td>\n",
       "      <td>0.00646</td>\n",
       "      <td>0.00602</td>\n",
       "      <td>0.00736</td>\n",
       "      <td>0.00212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>46.42001</td>\n",
       "      <td>6.27852</td>\n",
       "      <td>7.80921</td>\n",
       "      <td>-19.31205</td>\n",
       "      <td>-6.26033</td>\n",
       "      <td>0.29103</td>\n",
       "      <td>-0.43890</td>\n",
       "      <td>-2.03942</td>\n",
       "      <td>50.38709</td>\n",
       "      <td>5.99440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03460</td>\n",
       "      <td>0.10638</td>\n",
       "      <td>0.13328</td>\n",
       "      <td>0.03989</td>\n",
       "      <td>0.02011</td>\n",
       "      <td>0.01653</td>\n",
       "      <td>0.01174</td>\n",
       "      <td>0.01674</td>\n",
       "      <td>0.00972</td>\n",
       "      <td>0.01568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>47.42587</td>\n",
       "      <td>58.66367</td>\n",
       "      <td>18.29957</td>\n",
       "      <td>22.44312</td>\n",
       "      <td>3.44732</td>\n",
       "      <td>9.73675</td>\n",
       "      <td>20.38870</td>\n",
       "      <td>28.12449</td>\n",
       "      <td>48.93017</td>\n",
       "      <td>61.48818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00196</td>\n",
       "      <td>0.01685</td>\n",
       "      <td>0.02758</td>\n",
       "      <td>0.01583</td>\n",
       "      <td>0.01352</td>\n",
       "      <td>0.01092</td>\n",
       "      <td>0.01035</td>\n",
       "      <td>0.01155</td>\n",
       "      <td>0.01155</td>\n",
       "      <td>0.01210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-472.14160</td>\n",
       "      <td>-464.98404</td>\n",
       "      <td>-266.10935</td>\n",
       "      <td>-260.74717</td>\n",
       "      <td>0.52358</td>\n",
       "      <td>-0.31641</td>\n",
       "      <td>-2.38350</td>\n",
       "      <td>-1.96267</td>\n",
       "      <td>-469.34317</td>\n",
       "      <td>-462.24141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01152</td>\n",
       "      <td>0.00695</td>\n",
       "      <td>0.08807</td>\n",
       "      <td>0.01271</td>\n",
       "      <td>0.01075</td>\n",
       "      <td>0.00508</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.00227</td>\n",
       "      <td>0.01382</td>\n",
       "      <td>0.01333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>48.10987</td>\n",
       "      <td>-14.73357</td>\n",
       "      <td>8.10896</td>\n",
       "      <td>-5.33795</td>\n",
       "      <td>-72.41243</td>\n",
       "      <td>-37.06827</td>\n",
       "      <td>-13.87233</td>\n",
       "      <td>-17.01598</td>\n",
       "      <td>132.22925</td>\n",
       "      <td>28.24232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02223</td>\n",
       "      <td>0.05923</td>\n",
       "      <td>0.08215</td>\n",
       "      <td>0.02083</td>\n",
       "      <td>0.01408</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>0.00536</td>\n",
       "      <td>0.00333</td>\n",
       "      <td>0.00232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>6.47831</td>\n",
       "      <td>11.07881</td>\n",
       "      <td>8.28208</td>\n",
       "      <td>6.97798</td>\n",
       "      <td>15.12695</td>\n",
       "      <td>2.98742</td>\n",
       "      <td>2.43371</td>\n",
       "      <td>2.85373</td>\n",
       "      <td>2.82750</td>\n",
       "      <td>11.69582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.02071</td>\n",
       "      <td>0.00228</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>0.00202</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>0.00359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-516.30650</td>\n",
       "      <td>-499.31430</td>\n",
       "      <td>-414.98651</td>\n",
       "      <td>-409.20710</td>\n",
       "      <td>11.54804</td>\n",
       "      <td>11.62283</td>\n",
       "      <td>13.64184</td>\n",
       "      <td>13.52730</td>\n",
       "      <td>-521.90117</td>\n",
       "      <td>-505.42540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02330</td>\n",
       "      <td>0.05903</td>\n",
       "      <td>0.07646</td>\n",
       "      <td>0.02452</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.00787</td>\n",
       "      <td>0.00791</td>\n",
       "      <td>0.01267</td>\n",
       "      <td>0.00658</td>\n",
       "      <td>0.00627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>8.35603</td>\n",
       "      <td>6.60759</td>\n",
       "      <td>5.36747</td>\n",
       "      <td>5.04789</td>\n",
       "      <td>27.00908</td>\n",
       "      <td>4.60856</td>\n",
       "      <td>-0.66544</td>\n",
       "      <td>-1.27185</td>\n",
       "      <td>-27.57376</td>\n",
       "      <td>-5.39162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00278</td>\n",
       "      <td>0.00399</td>\n",
       "      <td>0.03365</td>\n",
       "      <td>0.00297</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00728</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00352</td>\n",
       "      <td>0.00283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-421.01856</td>\n",
       "      <td>-332.73392</td>\n",
       "      <td>-201.74638</td>\n",
       "      <td>-197.73655</td>\n",
       "      <td>-8.10179</td>\n",
       "      <td>5.86159</td>\n",
       "      <td>-22.22131</td>\n",
       "      <td>-16.83117</td>\n",
       "      <td>-420.09499</td>\n",
       "      <td>-336.18233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00970</td>\n",
       "      <td>0.03390</td>\n",
       "      <td>0.04437</td>\n",
       "      <td>0.01366</td>\n",
       "      <td>0.00860</td>\n",
       "      <td>0.00418</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.00375</td>\n",
       "      <td>0.00237</td>\n",
       "      <td>0.00435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>34.65005</td>\n",
       "      <td>-1.82677</td>\n",
       "      <td>7.09979</td>\n",
       "      <td>-0.38086</td>\n",
       "      <td>-12.23043</td>\n",
       "      <td>24.44729</td>\n",
       "      <td>-2.09877</td>\n",
       "      <td>-2.93850</td>\n",
       "      <td>49.63852</td>\n",
       "      <td>-19.76505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>0.00619</td>\n",
       "      <td>0.09354</td>\n",
       "      <td>0.00987</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>0.00326</td>\n",
       "      <td>0.00347</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>0.00453</td>\n",
       "      <td>0.00095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 988 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lag1_mean_0  lag1_mean_1  lag1_mean_2  lag1_mean_3  lag1_mean_d_h2h1_0  \\\n",
       "772      13.39603      5.96764      8.02252      8.37353             4.15383   \n",
       "1047     46.42001      6.27852      7.80921    -19.31205            -6.26033   \n",
       "72       47.42587     58.66367     18.29957     22.44312             3.44732   \n",
       "319    -472.14160   -464.98404   -266.10935   -260.74717             0.52358   \n",
       "1084     48.10987    -14.73357      8.10896     -5.33795           -72.41243   \n",
       "...           ...          ...          ...          ...                 ...   \n",
       "453       6.47831     11.07881      8.28208      6.97798            15.12695   \n",
       "345    -516.30650   -499.31430   -414.98651   -409.20710            11.54804   \n",
       "813       8.35603      6.60759      5.36747      5.04789            27.00908   \n",
       "388    -421.01856   -332.73392   -201.74638   -197.73655            -8.10179   \n",
       "367      34.65005     -1.82677      7.09979     -0.38086           -12.23043   \n",
       "\n",
       "      lag1_mean_d_h2h1_1  lag1_mean_d_h2h1_2  lag1_mean_d_h2h1_3  \\\n",
       "772              1.34713             0.11239             1.34190   \n",
       "1047             0.29103            -0.43890            -2.03942   \n",
       "72               9.73675            20.38870            28.12449   \n",
       "319             -0.31641            -2.38350            -1.96267   \n",
       "1084           -37.06827           -13.87233           -17.01598   \n",
       "...                  ...                 ...                 ...   \n",
       "453              2.98742             2.43371             2.85373   \n",
       "345             11.62283            13.64184            13.52730   \n",
       "813              4.60856            -0.66544            -1.27185   \n",
       "388              5.86159           -22.22131           -16.83117   \n",
       "367             24.44729            -2.09877            -2.93850   \n",
       "\n",
       "      lag1_mean_q1_0  lag1_mean_q1_1  ...  freq_659_3  freq_669_3  freq_679_3  \\\n",
       "772         16.98297         9.55623  ...     0.02189     0.04148     0.05375   \n",
       "1047        50.38709         5.99440  ...     0.03460     0.10638     0.13328   \n",
       "72          48.93017        61.48818  ...     0.00196     0.01685     0.02758   \n",
       "319       -469.34317      -462.24141  ...     0.01152     0.00695     0.08807   \n",
       "1084       132.22925        28.24232  ...     0.02223     0.05923     0.08215   \n",
       "...              ...             ...  ...         ...         ...         ...   \n",
       "453          2.82750        11.69582  ...     0.00126     0.00204     0.02071   \n",
       "345       -521.90117      -505.42540  ...     0.02330     0.05903     0.07646   \n",
       "813        -27.57376        -5.39162  ...     0.00278     0.00399     0.03365   \n",
       "388       -420.09499      -336.18233  ...     0.00970     0.03390     0.04437   \n",
       "367         49.63852       -19.76505  ...     0.00141     0.00619     0.09354   \n",
       "\n",
       "      freq_689_3  freq_699_3  freq_709_3  freq_720_3  freq_730_3  freq_740_3  \\\n",
       "772      0.02114     0.01120     0.01481     0.00646     0.00602     0.00736   \n",
       "1047     0.03989     0.02011     0.01653     0.01174     0.01674     0.00972   \n",
       "72       0.01583     0.01352     0.01092     0.01035     0.01155     0.01155   \n",
       "319      0.01271     0.01075     0.00508     0.00930     0.00227     0.01382   \n",
       "1084     0.02083     0.01408     0.01084     0.01120     0.00536     0.00333   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "453      0.00228     0.00056     0.00241     0.00043     0.00202     0.00068   \n",
       "345      0.02452     0.01361     0.00787     0.00791     0.01267     0.00658   \n",
       "813      0.00297     0.00276     0.00025     0.00728     0.00396     0.00352   \n",
       "388      0.01366     0.00860     0.00418     0.00535     0.00375     0.00237   \n",
       "367      0.00987     0.00451     0.00326     0.00347     0.00369     0.00453   \n",
       "\n",
       "      freq_750_3  \n",
       "772      0.00212  \n",
       "1047     0.01568  \n",
       "72       0.01210  \n",
       "319      0.01333  \n",
       "1084     0.00232  \n",
       "...          ...  \n",
       "453      0.00359  \n",
       "345      0.00627  \n",
       "813      0.00283  \n",
       "388      0.00435  \n",
       "367      0.00095  \n",
       "\n",
       "[353 rows x 988 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47baf807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772     1.0\n",
       "1047    0.0\n",
       "72      1.0\n",
       "319     1.0\n",
       "1084    1.0\n",
       "       ... \n",
       "453     1.0\n",
       "345     0.0\n",
       "813     1.0\n",
       "388     0.0\n",
       "367     1.0\n",
       "Name: Label, Length: 353, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd8ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 988)]             0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (None, 988, 1)           0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 988, 256)          198912    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 988, 256)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 252928)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 252929    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 451,841\n",
      "Trainable params: 451,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model architecture\n",
    "\n",
    "# creates an input layer with the shape of the input data.\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# adds a new dimension to the tensor to make it 3D. \n",
    "# This is required for the GRU layer to work since it expects a 3D tensor with \n",
    "# shape [batch, timesteps, feature] as input\n",
    "expand_dims = tf.expand_dims(inputs, axis=2)\n",
    "\n",
    "\n",
    "# defines a GRU layer with 256 units and returns a sequence of outputs for each input time step. \n",
    "# The return_sequences=True argument ensures that the GRU returns a sequence rather than just the last output.\n",
    "gru = tf.keras.layers.GRU(256, return_sequences=True)(expand_dims)\n",
    "\n",
    "# returns 0 for negative input values and 1 for positive input values. \n",
    "relu = tf.keras.layers.ReLU()(gru)\n",
    "\n",
    "#flattens the output of the relu layer to a 2D tensor.\n",
    "# can be helpful and reduce the number of parameters in the model\n",
    "flatten = tf.keras.layers.Flatten()(relu)\n",
    "\n",
    "# #adds a dense layer with 1 output unit and a sigmoid activation function. \n",
    "# #This layer produces the final prediction.\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "print(model.summary())\n",
    "Model: \"functional_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c31855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bb188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:55:27.255408: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 24s 2s/step - loss: 2.0103 - accuracy: 0.6353 - auc: 0.7179 - val_loss: 1.1487 - val_accuracy: 0.7697 - val_auc: 0.8848\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 24s 2s/step - loss: 1.0499 - accuracy: 0.8100 - auc: 0.8527 - val_loss: 0.4998 - val_accuracy: 0.8424 - val_auc: 0.9287\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.4462 - accuracy: 0.8283 - auc: 0.9122 - val_loss: 0.4367 - val_accuracy: 0.8727 - val_auc: 0.9388\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.2582 - accuracy: 0.8875 - auc: 0.9580 - val_loss: 0.2340 - val_accuracy: 0.8788 - val_auc: 0.9705\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.1701 - accuracy: 0.9255 - auc: 0.9828 - val_loss: 0.1957 - val_accuracy: 0.9212 - val_auc: 0.9785\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.1466 - accuracy: 0.9362 - auc: 0.9874 - val_loss: 0.1655 - val_accuracy: 0.9394 - val_auc: 0.9868\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.1100 - accuracy: 0.9650 - auc: 0.9936 - val_loss: 0.1805 - val_accuracy: 0.9152 - val_auc: 0.9838\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 23s 2s/step - loss: 0.1136 - accuracy: 0.9559 - auc: 0.9914 - val_loss: 0.1554 - val_accuracy: 0.9333 - val_auc: 0.9853\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 23s 2s/step - loss: 0.0851 - accuracy: 0.9666 - auc: 0.9962 - val_loss: 0.1540 - val_accuracy: 0.9394 - val_auc: 0.9878\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 23s 2s/step - loss: 0.0748 - accuracy: 0.9757 - auc: 0.9971 - val_loss: 0.1370 - val_accuracy: 0.9455 - val_auc: 0.9900\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.0558 - accuracy: 0.9833 - auc: 0.9986 - val_loss: 0.1762 - val_accuracy: 0.9030 - val_auc: 0.9904\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 24s 2s/step - loss: 0.0522 - accuracy: 0.9909 - auc: 0.9991 - val_loss: 0.1219 - val_accuracy: 0.9455 - val_auc: 0.9910\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.0408 - accuracy: 0.9878 - auc: 0.9995 - val_loss: 0.2034 - val_accuracy: 0.9212 - val_auc: 0.9891\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 24s 2s/step - loss: 0.0505 - accuracy: 0.9848 - auc: 0.9990 - val_loss: 0.1900 - val_accuracy: 0.9273 - val_auc: 0.9864\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.0297 - accuracy: 0.9909 - auc: 0.9998 - val_loss: 0.1769 - val_accuracy: 0.9273 - val_auc: 0.9888\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.0197 - accuracy: 0.9985 - auc: 1.0000 - val_loss: 0.1415 - val_accuracy: 0.9394 - val_auc: 0.9904\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 24s 2s/step - loss: 0.0094 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9455 - val_auc: 0.9893\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import tensorflow\n",
    "model.compile(\n",
    "    # purpose of optimzier is to changing the weights of the model to keep loss as low as possible\n",
    "    optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    class_weight={0: 1, 1: 1},\n",
    "    callbacks=[\n",
    "        # to prevent overfitting\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            # number of epochs to wait before stopping the training process if the validation loss does not improve\n",
    "            patience=5,\n",
    "            # restore the weights of the best epoch\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        tensorboard_callback\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f659a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.501%\n"
     ]
    }
   ],
   "source": [
    "model_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"Test Accuracy: {:.3f}%\".format(model_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c65f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 583ms/step\n",
      "Confusion Matrix:\n",
      " [[165  16]\n",
      " [ 14 158]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = np.round(model.predict(X_test))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\",(cm))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xticks(np.arange(2) + 0.5, label_mapping.keys())\n",
    "plt.yticks(np.arange(2) + 0.5, label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3906f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC score: 0.915103430553771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# assuming y_true and y_pred are your true and predicted binary labels respectively\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC-ROC score:\", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7cd4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba34fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelbci_demo/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelbci_demo/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "modelbci0225_json = model.to_json()\n",
    "with open(\"modelbci_demo.json\", \"w\") as json_file:\n",
    "    json_file.write(modelbci0225_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"modelbci_demo.h5\")\n",
    "model.save(\"modelbci_demo\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ba5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 4s 327ms/step\n",
      "12/12 [==============================] - 10s 761ms/step\n"
     ]
    }
   ],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"modelbci_demo\")\n",
    "\n",
    "# Let's check:\n",
    "np.testing.assert_allclose(model.predict(X_test), reconstructed_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59195f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 667ms/step\n",
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Relaxed       0.92      0.91      0.92       181\n",
      "Concentrated       0.91      0.92      0.91       172\n",
      "\n",
      "    accuracy                           0.92       353\n",
      "   macro avg       0.91      0.92      0.91       353\n",
      "weighted avg       0.92      0.92      0.92       353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test))\n",
    "clr = classification_report(y_test, y_pred, target_names=label_mapping.keys())\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd342ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "772     1.0\n",
      "1047    0.0\n",
      "72      1.0\n",
      "319     1.0\n",
      "1084    1.0\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "x_pred = np.round(model.predict(X_test[:5]))\n",
    "print(x_pred)\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464f132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
